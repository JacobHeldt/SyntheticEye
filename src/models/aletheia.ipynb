{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, WeightedRandomSampler, Subset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import Netron for visualizing our model\n",
    "import netron\n",
    "\n",
    "# Add scripts to directory\n",
    "sys.path.append('/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/src/utils')\n",
    "# Import custom helper functions from scripts directory\n",
    "import helper_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain Insights on Dataset\n",
    "This is so we can better understand our data and helps us to decide which fixed image size to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function from helper_functions.py\n",
    "from helper_functions import plot_image_dimensions_bar_graph\n",
    "from helper_functions import plot_total_image_dimensions_bar_graph\n",
    "from helper_functions import plot_class_distribution\n",
    "from helper_functions import check_accuracy_aletheia4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting overall image dimensions\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/\"\n",
    "plot_total_image_dimensions_bar_graph(img_dir, heading='Aletheia Dataset Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of cg-generated images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/AI/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='CG Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of GAN images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/GAN/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='GAN Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of real images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/REAL/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='Real Image Dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution('/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import show_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_directory, indices=None, transforms=None):\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        self.img_labels = []\n",
    "        self.img_names = []\n",
    "\n",
    "        # Iterate through classes\n",
    "        for class_id, class_name in enumerate(os.listdir(img_directory)):\n",
    "            class_dir = os.path.join(img_directory, class_name)\n",
    "            # Iterate through all images of a class\n",
    "            for img in os.listdir(class_dir):\n",
    "                if img.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "                    self.img_names.append(os.path.join(class_name, img))\n",
    "                    self.img_labels.append(class_id)\n",
    "\n",
    "        # Subset handling\n",
    "        if indices is not None:\n",
    "            self.img_names = [self.img_names[i] for i in indices]\n",
    "            self.img_labels = [self.img_labels[i] for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.img_directory, self.img_names[index])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            image = np.array(image) \n",
    "            image = self.transforms(image=image)['image']\n",
    "\n",
    "        label = self.img_labels[index]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_image_mean_std\n",
    "\n",
    "dataset_path = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/\"\n",
    "\n",
    "mean, std = get_image_mean_std(dataset_path)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data Augmentation\n",
    "We augment the images in our dataset to make sure our model is robust and to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304), \n",
    "    A.CenterCrop(256, 256), \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.03, rotate_limit=7, p=0.5),\n",
    "    A.PixelDropout(dropout_prob=0.01, p=0.35),\n",
    "    A.Normalize(mean=[0.4953, 0.4166, 0.3759], std=[0.2432, 0.2228, 0.2194]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304), \n",
    "    A.CenterCrop(256, 256), \n",
    "    A.Normalize(mean=[0.4953, 0.4166, 0.3759], std=[0.2432, 0.2228, 0.2194]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, images=12):\n",
    "    # Set up figure\n",
    "    fig, axes = plt.subplots(1, images, figsize=(images * 3, 3))\n",
    "    \n",
    "    for i in range(images):\n",
    "        # Get an image from dataset\n",
    "        image = dataset[i]\n",
    "        \n",
    "        # Convert image to numpy array\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "        # Display the image\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset = CustomImageDataset(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "print(f\"Number of samples in the dataset: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Weights for Classes in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3 \n",
    "\n",
    "def define_class_weights(labels, classes):\n",
    "    count = [0] * classes\n",
    "\n",
    "    # Count frequency of class labels\n",
    "    for label in labels:\n",
    "        count[label] += 1\n",
    "    class_weights = [0.] * classes\n",
    "\n",
    "    # Calculate number of samples in dataset\n",
    "    samples = float(sum(count))\n",
    "    \n",
    "    # Calculate weight for each class\n",
    "    for i in range(classes):\n",
    "        if count[i] == 0:\n",
    "            class_weights[i] = 0 \n",
    "        else:\n",
    "            class_weights[i] = samples / float(count[i])\n",
    "    weight = [class_weights[label] for label in labels]\n",
    "    return weight\n",
    "\n",
    "weights = define_class_weights(dataset.img_labels, classes)\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = WeightedRandomSampler(weights, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Split dataset into train, test, and validation sets\n",
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset_i, val_dataset_i, test_dataset_i = random_split(range(len(dataset)), [train_size, val_size, test_size])\n",
    "\n",
    "# Apply transforms and create dataset instances for each split\n",
    "transformed_train_dataset = CustomImageDataset(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\", indices=train_dataset_i, transforms=train_transforms)\n",
    "transformed_val_dataset = CustomImageDataset(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\", indices=val_dataset_i, transforms=test_transforms)\n",
    "transformed_test_dataset = CustomImageDataset(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\", indices=test_dataset_i, transforms=test_transforms)\n",
    "\n",
    "# Extract labels for the training set\n",
    "train_labels = [transformed_train_dataset.img_labels[i] for i in range(len(transformed_train_dataset))]\n",
    "\n",
    "# Calculate weights for the training set\n",
    "weights = define_class_weights(train_labels, classes)\n",
    "weights = torch.DoubleTensor(weights)\n",
    "\n",
    "# Create a sampler for the training set\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# Create DataLoaders for each dataset split\n",
    "train_loader = DataLoader(transformed_train_dataset, batch_size=32, sampler=sampler, shuffle=False)\n",
    "val_loader = DataLoader(transformed_val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(transformed_test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_train_dataset.__len__())\n",
    "print(transformed_val_dataset.__len__())\n",
    "print(transformed_test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Image With Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "train_transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304), \n",
    "    A.CenterCrop(256, 256), \n",
    "    A.HorizontalFlip(p=1),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=1),\n",
    "    A.PixelDropout(dropout_prob=0.015, p=1),\n",
    "])\n",
    "\n",
    "img = cv2.imread(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SampleData\\\\AI\\\\030CHMXYM3.jpg\") \n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Ensure image is in RGB format\n",
    "\n",
    "# Apply transformations\n",
    "transformed = train_transforms(image=img)\n",
    "transformed_img = transformed[\"image\"]\n",
    "\n",
    "# Plot original and transformed image \n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Transformed Image')\n",
    "plt.imshow(transformed_img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.1):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        branch_channels = out_channels // 2\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 5, 1, 2), \n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # Define skip connection\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply both branches\n",
    "        out3x3 = self.branch3x3(x)\n",
    "        out5x5 = self.branch5x5(x)\n",
    "\n",
    "        out = torch.cat([out3x3, out5x5], dim=1)\n",
    "\n",
    "        # Apply residual connection\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class Aletheia4Net(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.35):\n",
    "        super(Aletheia4Net, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 16),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(16, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(128, 256)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 3\n",
    "learning_rate = 0.0003\n",
    "batch_size = 32\n",
    "num_epochs = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility and initialize model\n",
    "torch.manual_seed(42)\n",
    "model = Aletheia4Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CrossEntropyLoss for classification problem\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# Use NAdam (a variant of the Adam optimizer) as our optimizer\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "# Define learning rate scheduler to automatically adjust our learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard\n",
    "writer = SummaryWriter(f'runs/Aletheia4_3')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import check_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aletheia4Net().to(device)\n",
    "\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\Development\\\\Aletheia4_3_epoch_16.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EmissionsTracker to track carbon emissions using the CodeCarbon library\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Aletheia4_3\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "metrics_interval = 100\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (inputs, labels) in train_progress:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % metrics_interval == 0:\n",
    "            writer.add_scalar('Training Loss', running_loss / metrics_interval, epoch * len(train_loader) + batch_idx)\n",
    "            writer.add_scalar('Training Accuracy', 100 * correct / total, epoch * len(train_loader) + batch_idx)\n",
    "            # Print training results\n",
    "            print(f'Epoch {epoch}, Training Accuracy: {100 * correct / total:.2f}%', flush=True)\n",
    "            # Reset values\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
    "\n",
    "    # Print validation results\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f'Aletheia4_3_epoch_{16 + epoch}.pth')\n",
    "\n",
    "# Finalize carbon tracking\n",
    "emissions = carbon_tracker.stop()\n",
    "print(f\"Emissions: {emissions:.5f} kgCO2eq\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aletheia4Net().to(device)\n",
    "\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\Development\\\\Aletheia4_3_epoch_16.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of trained model on the test data\n",
    "check_accuracy_aletheia4(train_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "new_root_directory = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\StyleGANEval\"\n",
    "\n",
    "new_dataset = CustomImageDataset(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\StyleGANEval\", transforms=test_transforms)\n",
    "\n",
    "new_test_loader = DataLoader(new_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate accuracy on new dataset\n",
    "check_accuracy_aletheia4(new_test_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aletheia4Net().to(device)\n",
    "\n",
    "# Perform forward pass to create the fc1 layer\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Load model\n",
    "model_path = \"./Aletheia4_3_epoch_0.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Ensure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Store predictions and labels\n",
    "all_preds = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  \n",
    "\n",
    "        # Collect predictions and true labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision = precision_score(all_true_labels, all_preds, average=None)\n",
    "\n",
    "# Print precision for each class\n",
    "for i, class_precision in enumerate(precision):\n",
    "    print(f\"Precision for class {i}: {class_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netron.start(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\Development\\\\Aletheia4_3_epoch_16.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
