{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add scripts to directory\n",
    "sys.path.append('/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/scripts')\n",
    "\n",
    "# Import custom helper functions from the scripts directory\n",
    "import helper_functions\n",
    "\n",
    "# For implementing different learning rates for different layers in the optimizer\n",
    "from itertools import chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dimensions of Images in Classes\n",
    "This helps us with:\n",
    "- Understanding our data\n",
    "- Choosing a fixed image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plot_image_dimensions function from helper_functions.py\n",
    "from helper_functions import plot_image_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jacob\\OneDrive\\Desktop\\SyntheticEye\\Development\\notebooks\\argus.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/notebooks/argus.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Plot dimensions of ai-generated images\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/notebooks/argus.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m img_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/jacob/OneDrive/Desktop/Unity Hub/image-dataset/dataset_1/Fake\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/notebooks/argus.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plot_image_dimensions(img_dir, heading\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFake Image Dimensions\u001b[39m\u001b[39m'\u001b[39m, alpha\u001b[39m=\u001b[39m\u001b[39m0.004\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users/jacob/OneDrive/Desktop/SyntheticEye/Development/scripts\\helper_functions.py:63\u001b[0m, in \u001b[0;36mplot_image_dimensions\u001b[1;34m(img_dir, heading, save_as, alpha)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mPlot the dimensions of images in a directory using a scatter plot.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39m# Getting all images in the directory\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m img_files \u001b[39m=\u001b[39m [f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(img_dir) \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith((\u001b[39m'\u001b[39m\u001b[39mjpeg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjpg\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpng\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwebp\u001b[39m\u001b[39m'\u001b[39m))]\n\u001b[0;32m     65\u001b[0m widths \u001b[39m=\u001b[39m []\n\u001b[0;32m     66\u001b[0m heights \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Plot dimensions of ai-generated images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Unity Hub/image-dataset/dataset_1/Fake\"\n",
    "plot_image_dimensions(img_dir, heading='Fake Image Dimensions', alpha=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dimensions of real images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Unity Hub/image-dataset/dataset_1/Real\"\n",
    "plot_image_dimensions(img_dir, heading='Fake Image Dimensions', alpha=0.004)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Network Architecture\n",
    "We will reuse the architecture from our face detection model. This architecture includes convolutional layers with residual blocks, followed by fully connected layers for classification.\n",
    "Using the same architecture allows us to utilize transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that consists of a convolutional block and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Define main convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Define skip connection and adapt channels\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    # Pass input through main block and add skip connection\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class AdjustedResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the architecture of our currently deployed face detection model (Aletheia). \n",
    "    We reuse this architceture for this general image detection model (Argus). \n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(AdjustedResCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling. The dropout probability is reduced for the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 24, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(24, 48, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(48, 96, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(96, 192, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 448),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(448, 224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(224, 112),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(112, 1) # Final layer with one output for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute size of flattend features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 224, 224)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        # Half the out_channels for each branch\n",
    "        branch_channels = out_channels // 2\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 5, 1, 2),  # Padding is 2 to keep the same spatial dimensions\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # Define skip connection and adapt channels\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply both branches\n",
    "        out3x3 = self.branch3x3(x)\n",
    "        out5x5 = self.branch5x5(x)\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        out = torch.cat([out3x3, out5x5], dim=1)\n",
    "\n",
    "        # Apply the residual connection\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class NewArgusNet(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(NewArgusNet, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 16, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(16, 32, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(32, 64, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(64, 128, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(128, 256, dropout_prob * 0.2)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "num_epochs = 36"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load datasets \n",
    "- Split data\n",
    "- Apply transformations to images and utilize data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images with albumentations transforms\n",
    "    \"\"\"\n",
    "    def __init__(self, img_paths, label_list, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Ensure the image is RGB\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB') \n",
    "        \n",
    "        img = np.array(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        label = self.label_list[index]\n",
    "        return img, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mean and Standard Deviation of Images\n",
    "Since transfer learning from Aletheia is used, we have to use the normalization values of the Aletheia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jacob\\OneDrive\\Desktop\\SyntheticEye\\Development\\notebooks\\argus.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/notebooks/argus.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhelper_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m get_image_mean_std\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/notebooks/argus.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_image_mean_std(\u001b[39m'\u001b[39m\u001b[39m/Users/jacob/OneDrive/Desktop/Unity Hub/image-dataset/dataset_1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users/jacob/OneDrive/Desktop/SyntheticEye/Development/scripts\\helper_functions.py:27\u001b[0m, in \u001b[0;36mget_image_mean_std\u001b[1;34m(dataset_path, device, batch_size)\u001b[0m\n\u001b[0;32m     21\u001b[0m image_transforms \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m     22\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m)),\n\u001b[0;32m     23\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor()\n\u001b[0;32m     24\u001b[0m ])\n\u001b[0;32m     26\u001b[0m \u001b[39m# Get image paths and labels from ImageFolder\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m img_folder \u001b[39m=\u001b[39m ImageFolder(root\u001b[39m=\u001b[39mdataset_path)\n\u001b[0;32m     28\u001b[0m img_paths \u001b[39m=\u001b[39m [item[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m img_folder\u001b[39m.\u001b[39mimgs]\n\u001b[0;32m     29\u001b[0m labels \u001b[39m=\u001b[39m [item[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m img_folder\u001b[39m.\u001b[39mimgs]\n",
      "File \u001b[1;32mc:\\Users\\jacob\\miniconda3\\envs\\env_two\\Lib\\site-packages\\torchvision\\datasets\\folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    302\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    303\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[39mstr\u001b[39m], \u001b[39mbool\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m ):\n\u001b[1;32m--> 309\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    310\u001b[0m         root,\n\u001b[0;32m    311\u001b[0m         loader,\n\u001b[0;32m    312\u001b[0m         IMG_EXTENSIONS \u001b[39mif\u001b[39;00m is_valid_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    313\u001b[0m         transform\u001b[39m=\u001b[39mtransform,\n\u001b[0;32m    314\u001b[0m         target_transform\u001b[39m=\u001b[39mtarget_transform,\n\u001b[0;32m    315\u001b[0m         is_valid_file\u001b[39m=\u001b[39mis_valid_file,\n\u001b[0;32m    316\u001b[0m     )\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\jacob\\miniconda3\\envs\\env_two\\Lib\\site-packages\\torchvision\\datasets\\folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtarget_transform)\n\u001b[0;32m    144\u001b[0m classes, class_to_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfind_classes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot)\n\u001b[1;32m--> 145\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_dataset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader \u001b[39m=\u001b[39m loader\n\u001b[0;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextensions \u001b[39m=\u001b[39m extensions\n",
      "File \u001b[1;32mc:\\Users\\jacob\\miniconda3\\envs\\env_two\\Lib\\site-packages\\torchvision\\datasets\\folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m class_to_idx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[39m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m make_dataset(directory, class_to_idx, extensions\u001b[39m=\u001b[39mextensions, is_valid_file\u001b[39m=\u001b[39mis_valid_file)\n",
      "File \u001b[1;32mc:\\Users\\jacob\\miniconda3\\envs\\env_two\\Lib\\site-packages\\torchvision\\datasets\\folder.py:87\u001b[0m, in \u001b[0;36mmake_dataset\u001b[1;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(target_dir):\n\u001b[0;32m     86\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m \u001b[39mfor\u001b[39;00m root, _, fnames \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39mwalk(target_dir, followlinks\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)):\n\u001b[0;32m     88\u001b[0m     \u001b[39mfor\u001b[39;00m fname \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(fnames):\n\u001b[0;32m     89\u001b[0m         path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, fname)\n",
      "File \u001b[1;32m<frozen os>:368\u001b[0m, in \u001b[0;36m_walk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helper_functions import get_image_mean_std\n",
    "get_image_mean_std('/Users/jacob/OneDrive/Desktop/Unity Hub/image-dataset/dataset_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4709, 0.4373, 0.3948]\n",
    "std = [0.2306, 0.2249, 0.2179]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Transformations\n",
    "Utilize data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    \"\"\"\n",
    "    Class to transform images using the Albumentations library\n",
    "    \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply Albumentations transform to the input image and convert the result to a tensor.\n",
    "    def __call__(self, img):\n",
    "        # Convert image to numpy array\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # Convert augmented image to a tensor and normalize pixel values\n",
    "        augmented = self.transform(image=img)\n",
    "        img_tensor = torch.from_numpy(augmented['image'].transpose(2, 0, 1)).float()  / 255.0\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionBridge:\n",
    "    \"\"\"\n",
    "    Bridge to convert Torchvision transforms into a format that can be used with the Albumentations library.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply torchvision transformations and convert the results to a numpy array\n",
    "    def __call__(self, img):\n",
    "        img = self.transform(img)\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformations\n",
    "\n",
    "# Albumentations transformations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.06, contrast_limit=0.06, p=0.3),\n",
    "    A.Rotate(limit=5, p=0.4),\n",
    "    A.PixelDropout(dropout_prob=0.014, p=0.3),\n",
    "    A.Normalize(mean=mean, std=std), \n",
    "])\n",
    "# Torchvision transformations\n",
    "train_torchvision_transform = transforms.Compose([\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomAffine(degrees=8, translate=(0.10, 0.10))\n",
    "    ], p=0.5),\n",
    "    transforms.Resize((256, 256)) \n",
    "])\n",
    "\n",
    "# Test Transformations\n",
    "\n",
    "# Albumentations transformations\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(mean=mean, std=std), \n",
    "])\n",
    "\n",
    "# Torchvision transformations\n",
    "test_torchvision_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256))\n",
    "])\n",
    "\n",
    "# This is used when testing the model on a specific source\n",
    "test_augmentation = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Custom Dataset class is used to incorporate Torchvision and Albumentation transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize CustomDataset Object\n",
    "    def __init__(self, dataset, torchvision_transforms=None, albumentations_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.torchvision_transforms = torchvision_transforms\n",
    "        self.albumentations_transform = AlbumentationsTransform(albumentations_transform)\n",
    "\n",
    "    # Return number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Check if dataset is a subset and get the path and target accordingly\n",
    "        if isinstance(self.dataset, torch.utils.data.Subset):\n",
    "            path, target = self.dataset.dataset.samples[self.dataset.indices[index]]\n",
    "        else:\n",
    "            path, target = self.dataset.samples[index]\n",
    "        \n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Apply torchvision transforms\n",
    "        if self.torchvision_transforms:\n",
    "            img = self.torchvision_transforms(img)\n",
    "        \n",
    "        # Apply albumentations transforms\n",
    "        if self.albumentations_transform:\n",
    "            img = self.albumentations_transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/Users/jacob/OneDrive/Desktop/Unity Hub/image-dataset/dataset_1'\n",
    "\n",
    "# Load dataset without transformations\n",
    "full_dataset = datasets.ImageFolder(root=root_directory)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.87 * len(full_dataset))  # 87%\n",
    "validation_size = int(0.04 * len(full_dataset))  # 4%\n",
    "test_size = len(full_dataset) - train_size - validation_size  # 9%\n",
    "\n",
    "train_subset, validation_subset, test_subset = random_split(full_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "# Apply transformations using the CustomDataset class\n",
    "train_dataset = CustomDataset(\n",
    "    train_subset, \n",
    "    torchvision_transforms=train_torchvision_transform, \n",
    "    albumentations_transform=train_transform\n",
    ")\n",
    "\n",
    "# Apply transformations to validation dataset\n",
    "val_dataset = CustomDataset(\n",
    "    validation_subset, \n",
    "    torchvision_transforms=test_torchvision_transform, \n",
    "    albumentations_transform=test_transform\n",
    ")\n",
    "\n",
    "# Apply transformations to test dataset\n",
    "test_dataset = CustomDataset(\n",
    "    test_subset, \n",
    "    torchvision_transforms=test_torchvision_transform, \n",
    "    albumentations_transform=test_transform\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import check_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewArgusNet(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (residual): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ResBlock(\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (residual): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): ResBlock(\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (residual): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): ResBlock(\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (residual): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): ResBlock(\n",
       "      (branch3x3): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (branch5x5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01)\n",
       "        (3): Dropout(p=0.06, inplace=False)\n",
       "      )\n",
       "      (residual): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Initialize model and transfer it to the GPU if available\n",
    "model = NewArgusNet().to(device)\n",
    "# Load trained model for transfer learning (from Alteheia 2.5)\n",
    "model.load_state_dict(torch.load(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Argus\\\\ar3_0_epoch_10_correct275512.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for our binary classification problem\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard summary writer\n",
    "writer = SummaryWriter(f'runs/Argus3_0')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using transfer learning and training different layers with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Group Layers\n",
    "early_conv_layers = list(model.conv_layers.children())[:3]  \n",
    "middle_conv_layers = list(model.conv_layers.children())[3:6]  \n",
    "fc_layers = list(model.fc_layers.children())\n",
    "\n",
    "# Create Parameter Groups for different learning rates\n",
    "param_groups = [\n",
    "    {'params': chain(*[layer.parameters() for layer in early_conv_layers]), 'lr': 0.000001},\n",
    "    {'params': chain(*[layer.parameters() for layer in middle_conv_layers]), 'lr': 0.00001},\n",
    "    {'params': chain(*[layer.parameters() for layer in fc_layers]), 'lr': 0.0001},\n",
    "]\n",
    "\n",
    "# Initialize Optimizer with Parameter Groups\n",
    "optimizer = torch.optim.NAdam(param_groups)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "    \n",
    "# Initialize EmissionsTracker to monitor carbon emissions using CodeCarbon\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Argus2_5_Emissions\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 200\n",
    "\n",
    "# Initialize variables for accuracy and loss values\n",
    "training_loss = 0.0\n",
    "training_accuracy = 0.0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset accuracy counters at beginning of each epoch\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train model on each batch of train_loader\n",
    "    for batch_index, (data, targets) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Progress in epoch\"):\n",
    "        # Move data and targets to the device\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        scores = scores.squeeze(1)\n",
    "        loss = loss_function(scores.view(-1), targets.float())  # Compute loss based on model's predictions\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert to binary decisions\n",
    "        preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "\n",
    "        # Update accuracy counters\n",
    "        correct += (preds == targets).sum().item()\n",
    "        samples += preds.size(0)\n",
    "        # Calculate accuracy as a percentage\n",
    "        accuracy = 100 * correct / samples\n",
    "\n",
    "        if batch_index % metrics_interval == 0:\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_index)\n",
    "            writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_index)\n",
    "            # Print metrics\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print accumulated accuracy for epoch\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(f'Got {correct} / {samples} correct with accuracy of {accuracy:.2f}% on training data.')\n",
    "\n",
    "    # Evaluate model on validation set after epoch\n",
    "    model.eval()\n",
    "    correct_test, samples_test = check_accuracy(val_loader, model, device)\n",
    "    test_accuracy = 100 * float(correct_test) / float(samples_test)\n",
    "    \n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "    \n",
    "    # Save state of model after epoch\n",
    "    torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Argus/Argus2Beta/ar2_5_epoch_{epoch}_correct{correct}.pth')\n",
    "\n",
    "# Stop EmissionsTracker\n",
    "emissions = carbon_tracker.stop()\n",
    "\n",
    "# Display total carbon emissions\n",
    "print(f\"Emissions: {emissions:.6f} kgCO2eq\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Without Transfer Learning\n",
    "We want to experiment with different methods for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "# model = NewArgusNet().to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Summary writer for TensorBoard\n",
    "writer = SummaryWriter(f'runs/ar_3_0')\n",
    "step = 0\n",
    "\n",
    "# Initialize Optimizer with single learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Argus3_0\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 200\n",
    "\n",
    "# Initialize variables for accuracy and loss values\n",
    "training_loss = 0.0\n",
    "training_accuracy = 0.0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset accuracy counters at beginning of each epoch\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train model on each batch of train_loader\n",
    "    for batch_index, (data, targets) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Progress in epoch\"):\n",
    "        # Move data and targets to the device\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        scores = scores.squeeze(1)\n",
    "        loss = loss_function(scores.view(-1), targets.float())  # Compute loss based on model's predictions\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert to binary decisions\n",
    "        preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "\n",
    "        # Update accuracy counters\n",
    "        correct += (preds == targets).sum().item()\n",
    "        samples += preds.size(0)\n",
    "        # Calculate accuracy as a percentage\n",
    "        accuracy = 100 * correct / samples\n",
    "\n",
    "        if batch_index % metrics_interval == 0:\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_index)\n",
    "            writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_index)\n",
    "            # Print metrics\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print accumulated accuracy for epoch\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(f'Got {correct} / {samples} correct with accuracy of {accuracy:.2f}% on training data.')\n",
    "\n",
    "    # Evaluate model on validation set after epoch\n",
    "    model.eval()\n",
    "    correct_test, samples_test = check_accuracy(val_loader, model, device)\n",
    "    test_accuracy = 100 * float(correct_test) / float(samples_test)\n",
    "    \n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "    \n",
    "    # Save state of model after epoch\n",
    "    torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/SyntheticEye/SyntheticEyeLocal/StateDicts/Argus/ar3_0_epoch_{epoch}_correct{correct}.pth')\n",
    "\n",
    "\n",
    "emissions = carbon_tracker.stop()\n",
    "print(f\"Emissions: {emissions:.6f} kgCO2eq\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NewArgusNet().to(device)\n",
    "\n",
    "# Create a dummy input and perform a forward pass to create the fc1 layer\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Argus\\\\ar3_0_epoch_10_correct275512.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(train_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import predict_single_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability for image: 0.9997989535331726\n"
     ]
    }
   ],
   "source": [
    "img_path = \"/Users/jacob/OneDrive/Desktop/OIG4.jpg\"\n",
    "predicted_label = predict_single_image(\n",
    "    img_path, \n",
    "    model,\n",
    "    # Use Albumentations to transform the image\n",
    "    AlbumentationsTransform(test_augmentation)\n",
    ")\n",
    "print(f\"Predicted probability for image: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Specific Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "new_root_directory = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Dalle3_1Eval\"\n",
    "\n",
    "new_full_dataset = datasets.ImageFolder(root=new_root_directory)\n",
    "\n",
    "# Apply data augmentation and images transformations\n",
    "\n",
    "new_test_dataset = CustomDataset(\n",
    "    new_full_dataset, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the new dataset\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate accuracy on new dataset\n",
    "check_accuracy(new_test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_transforms():\n",
    "    \"\"\"\n",
    "    Combine torchvision and albumentations transforms for an individual image\n",
    "    \"\"\"\n",
    "\n",
    "    tv_transform = tv_transform = TorchvisionBridge(torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224))]))\n",
    "    alb_transform = AlbumentationsTransform(test_augmentation)\n",
    "    \n",
    "    # Apply both transformations to the given image\n",
    "    def combined_transforms(img):\n",
    "        img = tv_transform(img)\n",
    "        return alb_transform(img)\n",
    "    \n",
    "    return combined_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for displaying predictions on multiple images\n",
    "from helper_functions import display_folder_images\n",
    "# Load combined transformations and display images with their predicted probabilities\n",
    "combined_transforms = single_image_transforms()\n",
    "display_folder_images(\"C:/Users/jacob/OneDrive/Dalle3_1Eval/\", model, combined_transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
