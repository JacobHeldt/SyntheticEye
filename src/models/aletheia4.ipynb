{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, WeightedRandomSampler, Subset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import Netron for visualizing our model\n",
    "import netron\n",
    "\n",
    "# # Add scripts to directory\n",
    "sys.path.append('/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/src/utils')\n",
    "# Import custom helper functions from the scripts directory\n",
    "import helper_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain Insights Regarding Data\n",
    "This this is so we can better understand our data and helps us to decide which fixed image size to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function from helper_functions.py\n",
    "from helper_functions import plot_image_dimensions_bar_graph\n",
    "from helper_functions import plot_class_distribution\n",
    "from helper_functions import check_accuracy_aletheia4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/AI/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='AI Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of GAN images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/GAN/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='GAN Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of real images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/REAL/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='Real Image Dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution('/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from helper_functions import show_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.img_labels = []\n",
    "        self.img_names = []\n",
    "\n",
    "        # Iterate through all classes\n",
    "        for class_id, class_name in enumerate(os.listdir(img_dir)):\n",
    "            class_dir = os.path.join(img_dir, class_name)\n",
    "            # Iterate through all images of a class\n",
    "            for img in os.listdir(class_dir):\n",
    "                if img.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "                    self.img_names.append(os.path.join(class_name, img))\n",
    "                    self.img_labels.append(class_id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names) # Length of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            image = np.array(image)  # Convert PIL image to numpy array\n",
    "            image = self.transforms(image=image)['image']  # Apply albumentations transforms\n",
    "\n",
    "        label = self.img_labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Data Augmentation\n",
    "We augment the images in our dataset to make sure they are robust and to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304), \n",
    "    A.CenterCrop(256, 256), \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "    A.PixelDropout(dropout_prob=0.015, p=0.35),\n",
    "    A.Normalize(mean=[0.499, 0.415, 0.372], std=[0.245, 0.223, 0.220]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304), \n",
    "    A.CenterCrop(256, 256), \n",
    "    A.Normalize(mean=[0.499, 0.415, 0.372], std=[0.245, 0.223, 0.220]),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_images=12):\n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Get an image from the dataset\n",
    "        image = dataset[i]\n",
    "        \n",
    "        # If the image is a tensor, convert it to a numpy array\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "        # Display the image\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset = CustomImageDataset(img_dir=\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(dataset)\n",
    "print(f\"Number of samples in the dataset: {num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for classes in dataset\n",
    "\n",
    "classes = 3 # Number of classes in dataset\n",
    "\n",
    "def define_class_weights(labels, classes):\n",
    "    count = [0] * classes\n",
    "\n",
    "    # Count frequency of class labels\n",
    "    for label in labels:\n",
    "        count[label] += 1\n",
    "    class_weights = [0.] * classes\n",
    "\n",
    "    # Calculate number of samples in dataset\n",
    "    samples = float(sum(count))\n",
    "\n",
    "    # Calculate weight for each class\n",
    "    for i in range(classes):\n",
    "        if count[i] == 0:\n",
    "            class_weights[i] = 0 \n",
    "        else:\n",
    "            class_weights[i] = samples / float(count[i])\n",
    "    weight = [class_weights[label] for label in labels]\n",
    "    return weight\n",
    "\n",
    "weights = define_class_weights(dataset.img_labels, classes)\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = WeightedRandomSampler(weights, len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed to ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Split dataset into train, test, and validation sets\n",
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = int(0.05 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Apply transforms by creating new instances of CustomImageDataset with the appropriate subset and transform\n",
    "transformed_train_dataset = CustomImageDataset(img_dir=\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\", transform=train_transforms)\n",
    "transformed_val_dataset = CustomImageDataset(img_dir=\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\", transform=test_transforms)\n",
    "transformed_test_dataset = CustomImageDataset(img_dir=\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia4Dataset\", transform=test_transforms)\n",
    "\n",
    "# Extract labels for the training set\n",
    "train_labels = [dataset.img_labels[idx] for idx in train_dataset.indices]\n",
    "\n",
    "# Calculate weights for the training set\n",
    "weights = define_class_weights(train_labels, classes)\n",
    "weights = torch.DoubleTensor(weights)\n",
    "\n",
    "# Create a sampler for the training set\n",
    "sampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# Now create the DataLoaders with the transformed datasets\n",
    "train_loader = DataLoader(transformed_train_dataset, batch_size=32, sampler=sampler, shuffle=False)\n",
    "val_loader = DataLoader(transformed_val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(transformed_test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Networks\n",
    "We experimented with multiple model architectures. The \"AdjustedResCNN\" is the architecture of Aletheia 2.5 and currently in use on our website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        # Half the out_channels for each branch\n",
    "        branch_channels = out_channels // 2\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 5, 1, 2), \n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # Define skip connection and adapt channels\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply both branches\n",
    "        out3x3 = self.branch3x3(x)\n",
    "        out5x5 = self.branch5x5(x)\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        out = torch.cat([out3x3, out5x5], dim=1)\n",
    "\n",
    "        # Apply the residual connection\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class Aletheia4Net(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(Aletheia4Net, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 16),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(16, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(128, 256),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(256, 512)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        # Testing feature size with 256x256 input\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.0002\n",
    "batch_size = 32\n",
    "num_epochs = 36"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize model and transfer it to the GPU if available\n",
    "model = Aletheia4Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for our binary classification problem\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "# Define NAdam (a veriant of the Adam optimizer) as our optimizer\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "# Define learning rate scheduler to adjust our learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard summary writer\n",
    "writer = SummaryWriter(f'runs/Aletheia4_0')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function for checking accuracy of our model from helper_functions.py\n",
    "from helper_functions import check_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EmissionsTracker to tracker to monitor carbon emissions using the CodeCarbon library\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Aletheia4_0\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 100\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_progress = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for batch_idx, (inputs, labels) in train_progress:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % metrics_interval == 0:\n",
    "            writer.add_scalar('Training Loss', running_loss / metrics_interval, epoch * len(train_loader) + batch_idx)\n",
    "            writer.add_scalar('Training Accuracy', 100 * correct / total, epoch * len(train_loader) + batch_idx)\n",
    "            running_loss = 0.0\n",
    "            # Print training results\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Training Accuracy: {100 * correct / total:.2f}%', flush=True)\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Validation Accuracy', val_accuracy, epoch)\n",
    "\n",
    "    # Print validation results\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f'model_epoch_{epoch}.pth')\n",
    "\n",
    "# Finalize carbon tracking\n",
    "emissions = carbon_tracker.stop()\n",
    "print(f\"Emissions: {emissions:.5f} kgCO2eq\")\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test state_dict of Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aletheia4Net().to(device)\n",
    "\n",
    "# Create a dummy input and perform a forward pass to create the fc1 layer\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"./model_epoch_18.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of trained model on the test data\n",
    "check_accuracy_aletheia4(val_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model using Netron\n",
    "netron.start(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Aletheia\\\\3_0\\\\al3_0_epoch_35_correct240789.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Specific Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "new_root_directory = \"/Users/jacob/OneDrive/StyleGAN images\"\n",
    "\n",
    "new_full_dataset = datasets.ImageFolder(root=new_root_directory)\n",
    "\n",
    "# Apply data augmentation and images transformations\n",
    "new_test_dataset = CustomDataset(\n",
    "    new_full_dataset, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the new dataset\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate accuracy on new dataset\n",
    "check_accuracy(new_test_loader, model, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model that will be used to predict individual images\n",
    "model_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Aletheia\\\\3_0\\\\al3_0_epoch_33_correct240773.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_transforms():\n",
    "    \"\"\"\n",
    "    Combine torchvision and albumentations transforms for an individual image\n",
    "    \"\"\"\n",
    "\n",
    "    tv_transform = tv_transform = TorchvisionBridge(torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224))]))\n",
    "    alb_transform = AlbumentationsTransform(test_augmentation)\n",
    "    \n",
    "    # Apply both transformations to the given image\n",
    "    def combined_transforms(img):\n",
    "        img = tv_transform(img)\n",
    "        return alb_transform(img)\n",
    "    \n",
    "    return combined_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for getting predictions on a single image\n",
    "from helper_functions import predict_single_image\n",
    "# Import function for displaying predictions on multiple images\n",
    "from helper_functions import display_folder_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/Users/jacob/OneDrive/Desktop/clearlyai.webp\"\n",
    "\n",
    "# Use predict_single_imge function to predict a single image\n",
    "predicted_label = predict_single_image(\n",
    "    img_path, \n",
    "    model,\n",
    "    # Use Albumentations to transform the image as needed\n",
    "    AlbumentationsTransform(test_augmentation)\n",
    ")\n",
    "\n",
    "# Print prediction for the given image\n",
    "print(f\"Predicted probability for image: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined transformations and display images with their predicted probabilities\n",
    "combined_transforms = single_image_transforms()\n",
    "display_folder_images(\"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/EvalData/Aletheia/MultipleEval/\", model, combined_transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
