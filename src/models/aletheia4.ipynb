{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import Netron for visualizing our model\n",
    "import netron\n",
    "\n",
    "# # Add scripts to directory\n",
    "sys.path.append('/Users/jacob/OneDrive/Desktop/SyntheticEye/Development/src/utils')\n",
    "# Import custom helper functions from the scripts directory\n",
    "import helper_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain Insights Regarding Data\n",
    "This this is so we can better understand our data and helps us to decide which fixed image size to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function from helper_functions.py\n",
    "from helper_functions import plot_image_dimensions_bar_graph\n",
    "from helper_functions import plot_class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/AI/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='AI Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of GAN images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/GAN/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='GAN Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of real images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/REAL/\"\n",
    "plot_image_dimensions_bar_graph(img_dir, heading='Real Image Dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution('/Users/jacob/OneDrive/Desktop/Aletheia4Dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from helper_functions import show_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_names = [img for img in os.listdir(img_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.webp'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304), \n",
    "    A.CenterCrop(256, 256), \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "    A.PixelDropout(dropout_prob=0.015, p=0.35),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomImageDataset(img_dir='C:\\\\Users\\\\jacob\\\\OneDrive\\\\AugmentationTestImages', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_images=12):\n",
    "    # Set up the figure\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Get an image from the dataset\n",
    "        image = dataset[i]\n",
    "        \n",
    "        # If the image is a tensor, convert it to a numpy array\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "        # Display the image\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(dataset, num_images=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Networks\n",
    "We experimented with multiple model architectures. The \"AdjustedResCNN\" is the architecture of Aletheia 2.5 and currently in use on our website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        # Half the out_channels for each branch\n",
    "        branch_channels = out_channels // 2\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 5, 1, 2),  # Padding is 2 to keep the same spatial dimensions\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # Define skip connection and adapt channels\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply both branches\n",
    "        out3x3 = self.branch3x3(x)\n",
    "        out5x5 = self.branch5x5(x)\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        out = torch.cat([out3x3, out5x5], dim=1)\n",
    "\n",
    "        # Apply the residual connection\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class Aletheia4Net(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(Aletheia4Net, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 16),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(16, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(128, 256),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(256, 512)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        # Testing feature size with 304x304 input\n",
    "        return self.conv_layers(torch.zeros(1, 3, 304, 304)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.0002\n",
    "batch_size = 32\n",
    "num_epochs = 36"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load Data\n",
    "- Split Data\n",
    "- Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    \"\"\"\n",
    "    Class to transform images using the Albumentations library\n",
    "    \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply Albumentations transform to the input image and convert the result to a tensor.\n",
    "    def __call__(self, img):\n",
    "        # Convert image to numpy array if needed\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # Convert augmented image to a tensor and normalize pixel values\n",
    "        augmented = self.transform(image=img)\n",
    "        img_tensor = torch.from_numpy(augmented['image'].transpose(2, 0, 1)).float() / 255.0\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionBridge:\n",
    "    \"\"\"\n",
    "    Bridge to convert Torchvision transforms into a format that can be used with the Albumentations library.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply torchvision transformations and convert the results to a numpy array\n",
    "    def __call__(self, img):\n",
    "        img = self.transform(img)\n",
    "        return np.array(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Std of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_image_mean_std\n",
    "\n",
    "dataset_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/Aletheia/Dataset/\"\n",
    "\n",
    "mean, std = get_image_mean_std(dataset_path)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation for normalization\n",
    "mean = [0.499, 0.415, 0.372]\n",
    "std = [0.245, 0.223, 0.220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations using both Torchvision and Albumentations libraries.\n",
    "\n",
    "# Torchvision transforms\n",
    "torchvision_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=3, translate=(0.03, 0.03)), # This helps making the model more robust on mobile (since users usually upload screenshots wich have a different positional format than the actual image), but a high value can weaken the accuracy on desktop (when used in the original image resultion).  \n",
    "    transforms.Resize((304, 304))  # Resize all images to 304x304\n",
    "])\n",
    "\n",
    "torchvision_resize = transforms.Compose(\n",
    "    transforms.Resize((304, 304)),\n",
    ")\n",
    "\n",
    "# Albumentation transforms for data augmentation\n",
    "augmentation = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.06, contrast_limit=0.06, p=0.25),\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.04, rotate_limit=7, p=0.4),\n",
    "    A.PixelDropout(dropout_prob=0.014, p=0.35),\n",
    "    A.Normalize(mean=mean, std=std), \n",
    "])\n",
    "\n",
    "# Albumentation transforms for test data, where images are simply resized without additional transformation\n",
    "test_augmentation = A.Compose([\n",
    "    A.Resize(304, 304),\n",
    "    A.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Custom Dataset class is used to incorporate Torchvision and Albumentation transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize CustomDataset\n",
    "    def __init__(self, dataset, torchvision_transforms=None, albumentations_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.torchvision_transforms = torchvision_transforms\n",
    "        self.albumentations_transform = AlbumentationsTransform(albumentations_transform)\n",
    "\n",
    "    # Return number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Check if dataset is a subset and get the path and target accordingly\n",
    "        if isinstance(self.dataset, torch.utils.data.Subset):\n",
    "            path, target = self.dataset.dataset.samples[self.dataset.indices[index]]\n",
    "        else:\n",
    "            path, target = self.dataset.samples[index]\n",
    "        \n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Apply torchvision transforms if defined\n",
    "        if self.torchvision_transforms:\n",
    "            img = self.torchvision_transforms(img)\n",
    "        \n",
    "        # Apply albumentations transforms if defined\n",
    "        if self.albumentations_transform:\n",
    "            img = self.albumentations_transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/Users/jacob/OneDrive/Desktop/SyntheticEye/Aletheia/Dataset/'\n",
    "\n",
    "# Load dataset without transformations\n",
    "full_dataset = datasets.ImageFolder(root=root_directory)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(46)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.85 * len(full_dataset))  # 85%\n",
    "validation_size = int(0.05 * len(full_dataset))  # 5%\n",
    "test_size = len(full_dataset) - train_size - validation_size  # 10%\n",
    "\n",
    "train_subset, validation_subset, test_subset = random_split(full_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "# Apply transformations using the CustomDataset class\n",
    "train_dataset = CustomDataset(\n",
    "    train_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=augmentation\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(\n",
    "    validation_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    test_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Initialize model and transfer it to the GPU if available\n",
    "model = Aletheia3Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for our binary classification problem\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "# Define NAdam (a veriant of the Adam optimizer) as our optimizer\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "# Define learning rate scheduler to adjust our learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard summary writer\n",
    "writer = SummaryWriter(f'runs/Aletheia3_0')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function for checking accuracy of our model from helper_functions.py\n",
    "from helper_functions import check_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EmissionsTracker to tracker to monitor carbon emissions using the CodeCarbon library\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Aletheia3_0\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 250\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Reset accuracy counters at beginning of each epoch\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    \n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train model on each batch of the train_loader and display progress in current epoch using tqdm\n",
    "    for batch_index, (data, targets) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Progress in epoch\"):\n",
    "\n",
    "        # Move data and targets to the device\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        scores = scores.squeeze(1)\n",
    "        loss = loss_function(scores.view(-1), targets.float()) # Compute loss based on model's predictions\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert to binary decisions\n",
    "        preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "\n",
    "        # Update accuracy counters\n",
    "        correct += (preds == targets).sum().item()\n",
    "        samples += preds.size(0)\n",
    "        # Calculate accuracy as percentage\n",
    "        accuracy = 100 * correct / samples\n",
    "\n",
    "\n",
    "        if batch_index % metrics_interval == 0:\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_index)\n",
    "            writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_index)\n",
    "            # Print metrics\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "\n",
    "    # Print accumulated accuracy for epoch\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate the model on validation set after each epoch\n",
    "    print(\"Checking accuracy on Test Data\")\n",
    "    correct_test, samples_test = check_accuracy(val_loader, model)\n",
    "    test_accuracy = 100 * float(correct_test) / float(samples_test)\n",
    "\n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar('Test Accuracy', test_accuracy, epoch) \n",
    "\n",
    "    # Save state of model after each epcoh\n",
    "    torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/SyntheticEye/SyntheticEyeLocal/StateDicts/Aletheia/3_0/al3_0_epoch_{epoch}_correct{correct}.pth')\n",
    "\n",
    "# Stop EmmisionsTracker\n",
    "emissions = carbon_tracker.stop()\n",
    "\n",
    "# Display total carbon emissions\n",
    "print(f\"Emissions: {emissions:.5f} kgCO2eq\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test state_dict of Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aletheia3Net().to(device)\n",
    "\n",
    "# Create a dummy input and perform a forward pass to create the fc1 layer\n",
    "sample_input = torch.randn(1, 3, 304, 304).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Aletheia\\\\3_0\\\\al3_0_epoch_33_correct240773.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of trained model on the test data\n",
    "check_accuracy(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model using Netron\n",
    "netron.start(\"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Aletheia\\\\3_0\\\\al3_0_epoch_35_correct240789.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Specific Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "new_root_directory = \"/Users/jacob/OneDrive/StyleGAN images\"\n",
    "\n",
    "new_full_dataset = datasets.ImageFolder(root=new_root_directory)\n",
    "\n",
    "# Apply data augmentation and images transformations\n",
    "new_test_dataset = CustomDataset(\n",
    "    new_full_dataset, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the new dataset\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate accuracy on new dataset\n",
    "check_accuracy(new_test_loader, model, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model that will be used to predict individual images\n",
    "model_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SyntheticEyeLocal\\\\StateDicts\\\\Aletheia\\\\3_0\\\\al3_0_epoch_33_correct240773.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_transforms():\n",
    "    \"\"\"\n",
    "    Combine torchvision and albumentations transforms for an individual image\n",
    "    \"\"\"\n",
    "\n",
    "    tv_transform = tv_transform = TorchvisionBridge(torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224))]))\n",
    "    alb_transform = AlbumentationsTransform(test_augmentation)\n",
    "    \n",
    "    # Apply both transformations to the given image\n",
    "    def combined_transforms(img):\n",
    "        img = tv_transform(img)\n",
    "        return alb_transform(img)\n",
    "    \n",
    "    return combined_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for getting predictions on a single image\n",
    "from helper_functions import predict_single_image\n",
    "# Import function for displaying predictions on multiple images\n",
    "from helper_functions import display_folder_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/Users/jacob/OneDrive/Desktop/clearlyai.webp\"\n",
    "\n",
    "# Use predict_single_imge function to predict a single image\n",
    "predicted_label = predict_single_image(\n",
    "    img_path, \n",
    "    model,\n",
    "    # Use Albumentations to transform the image as needed\n",
    "    AlbumentationsTransform(test_augmentation)\n",
    ")\n",
    "\n",
    "# Print prediction for the given image\n",
    "print(f\"Predicted probability for image: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined transformations and display images with their predicted probabilities\n",
    "combined_transforms = single_image_transforms()\n",
    "display_folder_images(\"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/EvalData/Aletheia/MultipleEval/\", model, combined_transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
