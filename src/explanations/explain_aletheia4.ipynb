{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Concatenate(nn.Module):\n",
    "    def forward(self, x1, x2):\n",
    "        return torch.cat([x1, x2], dim=1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        branch_channels = out_channels // 2\n",
    "\n",
    "        # 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        # 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, branch_channels, 5, 1, 2), \n",
    "            nn.BatchNorm2d(branch_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "\n",
    "        def get_concatenated_features(self, x):\n",
    "          # Apply both branches\n",
    "          out3x3 = self.branch3x3(x)\n",
    "          out5x5 = self.branch5x5(x)\n",
    "\n",
    "          # Concatenate along channel dimension\n",
    "          out = torch.cat([out3x3, out5x5], dim=1)\n",
    "          return out\n",
    "\n",
    "        # Define skip connection and adapt channels\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "        self.concatenate = Concatenate()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply both branches\n",
    "        out3x3 = self.branch3x3(x)\n",
    "        out5x5 = self.branch5x5(x)\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        out = torch.cat([out3x3, out5x5], dim=1)\n",
    "\n",
    "        # Apply the residual connection\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class Aletheia4Net(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(Aletheia4Net, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 16),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(16, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(128, 256),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(256, 512)\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def get_last_resblock_output(self, x):\n",
    "        # Apply all conv_layers except the last ResBlock\n",
    "        for layer in self.conv_layers[:-1]:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Get concatenated features from the last ResBlock\n",
    "        return self.conv_layers[-1].get_concatenated_features(x)\n",
    "\n",
    "    def feature_size(self):\n",
    "        # Testing feature size with 256x256 input\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def last_conv_layer(self):\n",
    "        return self.conv_layers[-1].branch3x3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Aletheia4 model\n",
    "model = Aletheia4Net().to(device)\n",
    "\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"../../../SyntheticEyeLocal/StateDicts/Aletheia/4_0/model_epoch_18.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mean and std for normalizing images\n",
    "mean = [0.499, 0.415, 0.372]\n",
    "std = [0.245, 0.223, 0.220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentation Transforms\n",
    "test_albumentations_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304),\n",
    "    A.CenterCrop(256, 256),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Albumentation Transforms\n",
    "original_albumentations_transform = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=304),\n",
    "    A.CenterCrop(256, 256),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_test_transforms(image_path):\n",
    "    \"\"\"\n",
    "    Apply test transformations to a single image.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Convert PIL image to numpy array\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Apply Albumentations transforms (assuming test_albumentations_transform is an Albumentations transform)\n",
    "    transformed = test_albumentations_transform(image=img_np)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_original_transforms(image_path):\n",
    "    \"\"\"\n",
    "    Apply test transformations to a single image.\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Convert PIL image to numpy array\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Apply Albumentations transforms (assuming test_albumentations_transform is an Albumentations transform)\n",
    "    transformed = original_albumentations_transform(image=img_np)\n",
    "    transformed_image = transformed[\"image\"]\n",
    "\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and show image\n",
    "file_path = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\SampleData\\\\GAN\\\\1.jpg\"\n",
    "img = PIL.Image.open(file_path)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to image\n",
    "img_transformed = apply_test_transforms(file_path).unsqueeze(0) # The model will be evaluated on this image\n",
    "img_original = apply_original_transforms(file_path).unsqueeze(0) # The original resized image will be displayed und the gradient mask\n",
    "img_original = img_original.to(device)\n",
    "img_transformed = img_transformed.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Prediction on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to(device)\n",
    "scores = model(img_transformed)\n",
    "\n",
    "# Apply softmax to get probabilities for each class\n",
    "probabilities = torch.softmax(scores, dim=1).squeeze()\n",
    "\n",
    "# Get the predicted class (the one with the highest probability)\n",
    "predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "probabilities_list = probabilities.tolist()\n",
    "\n",
    "print(probabilities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, img_path, target_layer, class_of_interest=None):\n",
    "    # Load and preprocess the image\n",
    "    input_img = apply_test_transforms(img_path).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the model's feature and hook the target layer\n",
    "    feature_maps = None\n",
    "    def get_features_hook(module, input, output):\n",
    "        nonlocal feature_maps\n",
    "        feature_maps = output.detach()\n",
    "\n",
    "    hook = target_layer.register_forward_hook(get_features_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    scores = model(input_img)\n",
    "    probabilities = torch.softmax(scores, dim=1).squeeze()\n",
    "\n",
    "    # Choose the class of interest\n",
    "    if class_of_interest is None:\n",
    "        class_of_interest = torch.argmax(probabilities).item()\n",
    "\n",
    "    # Target for backprop\n",
    "    target = scores[0, class_of_interest]\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    target.backward()\n",
    "\n",
    "    # Hook removal\n",
    "    hook.remove()\n",
    "\n",
    "    # Get gradients and feature maps\n",
    "    gradients = target_layer.weight.grad\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "    # Weight the channels by corresponding gradients\n",
    "    for i in range(feature_maps.shape[1]):\n",
    "        feature_maps[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "    # Average the channels of the feature maps\n",
    "    grad_cam = torch.mean(feature_maps, dim=1).squeeze()\n",
    "\n",
    "    # ReLU on top of the heatmap\n",
    "    grad_cam = F.relu(grad_cam)\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    grad_cam = grad_cam / grad_cam.max()\n",
    "\n",
    "    return grad_cam.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM map\n",
    "grad_cam_map = generate_grad_cam(model, file_path, model.last_conv_layer)\n",
    "\n",
    "# Load and preprocess image for plotting\n",
    "original_img = Image.open(file_path)\n",
    "original_img = apply_original_transforms(file_path).cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Resize Grad-CAM map to the size of the original image\n",
    "grad_cam_map = np.uint8(255 * grad_cam_map)  # Convert to uint8\n",
    "grad_cam_map = np.array(Image.fromarray(grad_cam_map).resize((original_img.shape[1], original_img.shape[0]), PIL.Image.LANCZOS))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(original_img, alpha=0.6)\n",
    "plt.imshow(grad_cam_map, cmap='jet', alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntheticeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
