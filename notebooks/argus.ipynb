{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add scripts to directory\n",
    "sys.path.append('C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\Development\\\\scripts')\n",
    "# Import custom helper functions from the scripts directory\n",
    "import helper_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dimensions of Images in Classes\n",
    "This helps us with:\n",
    "- Understanding our data\n",
    "- Choosing a fixed image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function from helper_functions.py\n",
    "from helper_functions import plot_image_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/image-dataset/dataset_1/Fake\"\n",
    "plot_image_dimensions(img_dir, heading='Fake Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of real images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/image-dataset/dataset_1/Real\"\n",
    "plot_image_dimensions(img_dir, heading='Fake Image Dimensions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Network Architecture\n",
    "We will reuse the architecture from our face detection model. This architecture includes convolutional layers with residual blocks, followed by fully connected layers for classification.\n",
    "This is essential, because we want to utilize transfer learning with our trained face detection model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that consists of a convolutional block and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.2):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Define main convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Define skip connection and adapt channels as if needed\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    # Pass input through main block and add skip connection\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class AdjustedResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an adjusted version of our DeeperDNN with slight adjustments and the addition of skip connections.\n",
    "    This is the model architecture of our currently deployed face detection model.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(AdjustedResCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling. The dropout probability is reduced for the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 24, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(24, 48, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(48, 96, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(96, 192, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 448),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(448, 224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(224, 112),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(112, 1) # Final layer with one output for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute size of flattend features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 224, 224)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.00005\n",
    "batch_size = 16\n",
    "num_epochs = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load datasets \n",
    "- Split data\n",
    "- Apply transformations to images and utilize data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images with albumentations transforms\n",
    "    \"\"\"\n",
    "    def __init__(self, img_paths, label_list, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Ensure the image is RGB\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB') \n",
    "        \n",
    "        img = np.array(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        label = self.label_list[index]\n",
    "        return img, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mean and Standard Deviation of Images\n",
    "Since transfer learning from Aletheia is used, we have to use the normalization values of the Aletheia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation from Aletheia dataset, since transfer learning is used \n",
    "mean = [0.499, 0.415, 0.372]\n",
    "std = [0.245, 0.223, 0.220]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Transformations\n",
    "Utilize data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    \"\"\"\n",
    "    Class to transform images using the Albumentations library\n",
    "    \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply Albumentations transform to the input image and convert the result to a tensor.\n",
    "    def __call__(self, img):\n",
    "        # Convert image to numpy array if needed\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # Convert augmented image to a tensor and normalize pixel values\n",
    "        augmented = self.transform(image=img)\n",
    "        img_tensor = torch.from_numpy(augmented['image'].transpose(2, 0, 1)).float()  / 255.0\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionBridge:\n",
    "    \"\"\"\n",
    "    Bridge to convert Torchvision transforms into a format that can be used with the Albumentations library.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply torchvision transformations and convert the results to a numpy array\n",
    "    def __call__(self, img):\n",
    "        img = self.transform(img)\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformations\n",
    "\n",
    "# Albumentations transformations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.PixelDropout(dropout_prob=0.01, p=0.3),\n",
    "    A.Normalize(mean=mean, std=std), \n",
    "])\n",
    "# Torchvision transformations\n",
    "train_torchvision_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.025, 0.025)), # This helps making the model more robust on mobile (since users usually upload screenshots wich have a different positional format than the actual image), but a high value can weaken the accuracy on desktop (when used in the original image resultion).  \n",
    "    transforms.Resize((224, 224))  # Resize all images to 224x224\n",
    "])\n",
    "\n",
    "# Test Transformations\n",
    "\n",
    "# Albumentations transformations\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "])\n",
    "\n",
    "# Torchvision transformations\n",
    "test_torchvision_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Custom Dataset class is used to incorporate Torchvision and Albumentation transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize CustomDataset Object\n",
    "    def __init__(self, dataset, torchvision_transforms=None, albumentations_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.torchvision_transforms = torchvision_transforms\n",
    "        self.albumentations_transform = AlbumentationsTransform(albumentations_transform)\n",
    "\n",
    "    # Return number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Check if dataset is a subset and get the path and target accordingly\n",
    "        if isinstance(self.dataset, torch.utils.data.Subset):\n",
    "            path, target = self.dataset.dataset.samples[self.dataset.indices[index]]\n",
    "        else:\n",
    "            path, target = self.dataset.samples[index]\n",
    "        \n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Apply torchvision transforms if defined\n",
    "        if self.torchvision_transforms:\n",
    "            img = self.torchvision_transforms(img)\n",
    "        \n",
    "        # Apply albumentations transforms if defined\n",
    "        if self.albumentations_transform:\n",
    "            img = self.albumentations_transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/Users/jacob/OneDrive/Desktop/SyntheticEye/Version2_4/Dataset/'\n",
    "\n",
    "# Load dataset without transformations\n",
    "full_dataset = datasets.ImageFolder(root=root_directory)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.85 * len(full_dataset))  # 85%\n",
    "validation_size = int(0.05 * len(full_dataset))  # 5%\n",
    "test_size = len(full_dataset) - train_size - validation_size  # 10%\n",
    "\n",
    "train_subset, validation_subset, test_subset = random_split(full_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "# Apply transformations using the CustomDataset class\n",
    "train_dataset = CustomDataset(\n",
    "    train_subset, \n",
    "    torchvision_transforms=train_torchvision_transform, \n",
    "    albumentations_transform=train_transform\n",
    ")\n",
    "\n",
    "# Apply transformations to validation dataset\n",
    "val_dataset = CustomDataset(\n",
    "    validation_subset, \n",
    "    torchvision_transforms=test_torchvision_transform, \n",
    "    albumentations_transform=test_transform\n",
    ")\n",
    "\n",
    "# Apply transformations to test dataset\n",
    "test_dataset = CustomDataset(\n",
    "    test_subset, \n",
    "    torchvision_transforms=test_torchvision_transform, \n",
    "    albumentations_transform=test_transform\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few images from the train dataset\n",
    "from helper_functions import show_img\n",
    "show_img(train_loader, class_names=full_dataset.classes, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import check_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Initialize model and transfer it to the GPU (if available)\n",
    "model = AdjustedResCNN().to(device)\n",
    "# Load trained model for transfer learning (from Alteheia 2.5)\n",
    "model.load_state_dict(torch.load(\"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Aletheia/2_5/al2_5_epoch_16_correct235028.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for our binary classification problem\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "# Define NAdam (a variant of the Adam optimizer) as our optimizer\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "# Define learning rate scheduler to adjust our learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard summary writer\n",
    "writer = SummaryWriter(f'runs/Argus1_0')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "Using transfer learning and only training the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally freeze the parameters of all layers except the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze the parameters of the last layer\n",
    "for param in model.fc_layers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Initialize EmissionsTracker to tracker to monitor carbon emissions using the CodeCarbon library\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Argus2_5\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 100\n",
    "\n",
    "# Initialize variables for accuracy and loss values\n",
    "training_loss = 0.0\n",
    "training_accuracy = 0.0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset accuracy counters at the beginning of each epoch\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train model on each batch of the train_loader and display progress in current epoch using tqdm\n",
    "    for batch_index, (data, targets) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Progress in epoch\"):\n",
    "        # Move data and targets to the device\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        scores = scores.squeeze(1)\n",
    "        loss = loss_function(scores.view(-1), targets.float())  # Compute loss based on model's predictions\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert to binary decisions\n",
    "        preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "\n",
    "        # Update accuracy counters\n",
    "        correct += (preds == targets).sum().item()\n",
    "        samples += preds.size(0)\n",
    "        # Calculate accuracy as a percentage\n",
    "        accuracy = 100 * correct / samples\n",
    "\n",
    "        if batch_index % metrics_interval == 0:\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_index)\n",
    "            writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_index)\n",
    "            # Print metrics\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print accumulated accuracy for the epoch\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "    # Evaluate the model on the validation set after each epoch\n",
    "    model.eval()\n",
    "    correct_test, samples_test = check_accuracy(val_loader, model)\n",
    "    test_accuracy = 100 * float(correct_test) / float(samples_test)\n",
    "    \n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar('Test Accuracy', test_accuracy, epoch)\n",
    "    \n",
    "    # Save state of model after each epoch\n",
    "    torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Argus/Argus2/ar2_epoch_{epoch}_correct{correct}.pth')\n",
    "\n",
    "# Stop EmissionsTracker\n",
    "emissions = carbon_tracker.stop()\n",
    "\n",
    "# Display total carbon emissions\n",
    "print(f\"Emissions: {emissions:.10f} kgCO2eq\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(test_loader, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
