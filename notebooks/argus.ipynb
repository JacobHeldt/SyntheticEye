{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add scripts to directory\n",
    "sys.path.append('C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\Aletheia\\\\Aletheia\\\\scripts')\n",
    "# Import custom helper functions from the scripts directory\n",
    "import helper_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dimensions of Images in Classes\n",
    "This helps us with:\n",
    "- Understanding our data\n",
    "- Choosing a fixed image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/image-dataset/dataset_1/Fake\"\n",
    "\n",
    "# Getting all images in the directory that contains ai-generated faces\n",
    "img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('jpeg', 'jpg', 'png', 'webp'))]\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# Loop through each image and extract its dimensions\n",
    "for img_file in img_files:\n",
    "    with Image.open(os.path.join(img_dir, img_file)) as img:\n",
    "        width, height = img.size\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "\n",
    "# Plot the dimensions of the images in a scatter plot\n",
    "plt.scatter(widths, heights, alpha=0.1)\n",
    "plt.title('Fake Image Dimensions')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/image-dataset/dataset_1/Real\"\n",
    "\n",
    "# Getting all images in the directory that contains ai-generated faces\n",
    "img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('jpeg', 'jpg', 'png', 'webp'))]\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# Loop through each image and extract its dimensions\n",
    "for img_file in img_files:\n",
    "    with Image.open(os.path.join(img_dir, img_file)) as img:\n",
    "        width, height = img.size\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "\n",
    "# Plot the dimensions of the images in a scatter plot\n",
    "plt.scatter(widths, heights, alpha=0.1)\n",
    "plt.title('Fake Image Dimensions')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Network Architecture\n",
    "We will reuse the architecture from our face detection model. This architecture includes convolutional layers with residual blocks, followed by fully connected layers for classification.\n",
    "This is essential, because we want to utilize transfer learning with our trained face detection model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that consists of a convolutional block and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.2):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Define main convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Define skip connection and adapt channels as if needed\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    # Pass input through main block and add skip connection\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class AdjustedResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an adjusted version of our DeeperDNN with slight adjustments and the addition of skip connections.\n",
    "    This is the model architecture of our currently deployed face detection model.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(AdjustedResCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling. The dropout probability is reduced for the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 24, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(24, 48, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(48, 96, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(96, 192, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 448),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(448, 224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(224, 112),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(112, 1) # Final layer with one output for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute size of flattend features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load datasets \n",
    "- Split data\n",
    "- Apply transformations to images and utilize data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading images with albumentations transforms\n",
    "    \"\"\"\n",
    "    def __init__(self, img_paths, label_list, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_paths[index]\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Ensure the image is RGB\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB') \n",
    "        \n",
    "        img = np.array(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        label = self.label_list[index]\n",
    "        return img, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Image Transformations\n",
    "Utilize data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.Normalize(mean=[0, 0, 0], std=[255, 255, 255], max_pixel_value=255.0),  # This line normalizes to [0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Standard Deviation of Images\n",
    "This is helpful information for normalizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_image_mean_std\n",
    "\n",
    "dataset_path = \"/Users/jacob/OneDrive/Desktop/image-dataset/dataset_1\"\n",
    "\n",
    "mean, std = get_image_mean_std(dataset_path)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
