{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OS interaction and system-specific parameters\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import Netron for visualizing our model\n",
    "import netron\n",
    "\n",
    "# Add scripts to directory\n",
    "sys.path.append('C:\\\\Users\\\\jacob\\\\OneDrive\\\\Desktop\\\\SyntheticEye\\\\Development\\\\scripts')\n",
    "# Import custom helper functions from the scripts directory\n",
    "import helper_functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dimensions of Images\n",
    "This this is so we can better understand our data and helps us to decide which fixed image size to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function from helper_functions.py\n",
    "from helper_functions import plot_image_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/Version2_4/Dataset/Fake/\"\n",
    "plot_image_dimensions(img_dir, heading='Fake Image Dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of real images\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/Version2_4/Dataset/Real/\"\n",
    "plot_image_dimensions(img_dir, heading='Fake Image Dimensions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Networks\n",
    "We experimented with multiple model architectures. The \"AdjustedResCNN\" is the architecture of Aletheia 2.5 and currently in use on our website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simpler CNN model architecture we used in the beginning.\n",
    "    This model is fast, but its performance not always optimal.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, dropout_prob=0.3):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Applying dropout with lower probability for convolutional layers\n",
    "        self.dropout_conv = nn.Dropout(p=dropout_prob/3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 64)\n",
    "\n",
    "        # Dropout layer for fully connected layers\n",
    "        self.dropout_fc = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layer 1 with ReLU activation followed by MaxPooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) \n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Convolutional layer 2 with ReLU activation followed by MaxPooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A deeper CNN that's more complex than the original CNN.\n",
    "    This model can detect more features but is also slower.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Convolutional layer 1\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2), # A lower dropout probability for convolutional layers\n",
    "\n",
    "            # Convolutional layer 2\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2), # A lower dropout probability for convolutional layers\n",
    "\n",
    "            # Convolutional layer 3\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2), # A lower dropout probability for convolutional layers\n",
    "\n",
    "            # Convolutional layer 4\n",
    "            nn.Conv2d(128, 224, 3, 1, 1),\n",
    "            nn.BatchNorm2d(224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2) # A lower dropout probability for convolutional layers\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(512, 224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(224, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(128, 1)  # Final output layer for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute the size of flattened features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers.\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 224, 224)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that consists of a convolutional block and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.2):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Define main convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Define skip connection and adapt channels as if needed\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    # Pass input through main block and add skip connection\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class AdjustedResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an adjusted version of our DeeperDNN with slight adjustments and the addition of skip connections.\n",
    "    This is the model architecture of our currently deployed face detection model.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(AdjustedResCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling. The dropout probability is reduced for the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 24, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(24, 48, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(48, 96, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(96, 192, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 448),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(448, 224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(224, 112),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(112, 1) # Final layer with one output for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute size of flattend features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 224, 224)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.00075\n",
    "batch_size = 16\n",
    "num_epochs = 22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load Data\n",
    "- Split Data\n",
    "- Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    \"\"\"\n",
    "    Class to transform images using the Albumentations library\n",
    "    \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply Albumentations transform to the input image and convert the result to a tensor.\n",
    "    def __call__(self, img):\n",
    "        # Convert image to numpy array if needed\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # Convert augmented image to a tensor and normalize pixel values\n",
    "        augmented = self.transform(image=img)\n",
    "        img_tensor = torch.from_numpy(augmented['image'].transpose(2, 0, 1)).float() / 255.0\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionBridge:\n",
    "    \"\"\"\n",
    "    Bridge to convert Torchvision transforms into a format that can be used with the Albumentations library.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply torchvision transformations and convert the results to a numpy array\n",
    "    def __call__(self, img):\n",
    "        img = self.transform(img)\n",
    "        return np.array(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Std of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_image_mean_std\n",
    "\n",
    "dataset_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/Version2_4/Dataset/\"\n",
    "\n",
    "mean, std = get_image_mean_std(dataset_path)\n",
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation for normalization\n",
    "mean = [0.499, 0.415, 0.372]\n",
    "std = [0.245, 0.223, 0.220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations using both Torchvision and Albumentations libraries.\n",
    "\n",
    "# Torchvision transforms\n",
    "torchvision_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=3, translate=(0.03, 0.03)), # This helps making the model more robust on mobile (since users usually upload screenshots wich have a different positional format than the actual image), but a high value can weaken the accuracy on desktop (when used in the original image resultion).  \n",
    "    transforms.Resize((224, 224))  # Resize all images to 224x224\n",
    "])\n",
    "\n",
    "torchvision_resize = transforms.Compose(\n",
    "    transforms.Resize((224, 224)),\n",
    ")\n",
    "\n",
    "# Albumentation transforms for data augmentation\n",
    "augmentation = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.04, rotate_limit=6, p=0.4),\n",
    "    A.PixelDropout(dropout_prob=0.013, p=0.35),\n",
    "    A.Normalize(mean=mean, std=std), \n",
    "])\n",
    "\n",
    "# Albumentation transforms for test data, where images are simply resized without additional transformation\n",
    "test_augmentation = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This CustomDataset\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Custom Dataset class is used to incorporate Torchvision and Albumentation transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize CustomDataset Object\n",
    "    def __init__(self, dataset, torchvision_transforms=None, albumentations_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.torchvision_transforms = torchvision_transforms\n",
    "        self.albumentations_transform = AlbumentationsTransform(albumentations_transform)\n",
    "\n",
    "    # Return number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Check if dataset is a subset and get the path and target accordingly\n",
    "        if isinstance(self.dataset, torch.utils.data.Subset):\n",
    "            path, target = self.dataset.dataset.samples[self.dataset.indices[index]]\n",
    "        else:\n",
    "            path, target = self.dataset.samples[index]\n",
    "        \n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Apply torchvision transforms if defined\n",
    "        if self.torchvision_transforms:\n",
    "            img = self.torchvision_transforms(img)\n",
    "        \n",
    "        # Apply albumentations transforms if defined\n",
    "        if self.albumentations_transform:\n",
    "            img = self.albumentations_transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/Users/jacob/OneDrive/Desktop/SyntheticEye/Version2_4/Dataset/'\n",
    "\n",
    "# Load dataset without transformations\n",
    "full_dataset = datasets.ImageFolder(root=root_directory)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(46)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.85 * len(full_dataset))  # 85%\n",
    "validation_size = int(0.05 * len(full_dataset))  # 5%\n",
    "test_size = len(full_dataset) - train_size - validation_size  # 10%\n",
    "\n",
    "train_subset, validation_subset, test_subset = random_split(full_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "# Apply transformations using the CustomDataset class\n",
    "train_dataset = CustomDataset(\n",
    "    train_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=augmentation\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(\n",
    "    validation_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    test_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Initialize model and transfer it to the GPU (if available)\n",
    "model = AdjustedResCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for our binary classification problem\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "# Define NAdam (a veriant of the Adam optimizer) as our optimizer\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "# Define learning rate scheduler to adjust our learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard summary writer\n",
    "writer = SummaryWriter(f'runs/Aletheia2_5')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary function for checking accuracy of our model from helper_functions.py\n",
    "from helper_functions import check_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EmissionsTracker to tracker to monitor carbon emissions using the CodeCarbon library\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Aletheia2_5\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 250\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Reset accuracy counters at beginning of each epoch\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    \n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train model on each batch of the train_loader and display progress in current epoch using tqdm\n",
    "    for batch_index, (data, targets) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Progress in epoch\"):\n",
    "\n",
    "        # Move data and targets to the device\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        scores = scores.squeeze(1)\n",
    "        loss = loss_function(scores.view(-1), targets.float()) # Compute loss based on model's predictions\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert predictions to binary decisions\n",
    "        preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "\n",
    "        # Update accuracy counters\n",
    "        correct += (preds == targets).sum().item()\n",
    "        samples += preds.size(0)\n",
    "        # Calculate accuracy as percentage\n",
    "        accuracy = 100 * correct / samples\n",
    "\n",
    "\n",
    "        if batch_index % metrics_interval == 0:\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_index)\n",
    "            writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_index)\n",
    "            # Print metrics\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "\n",
    "    # Print accumulated accuracy for epoch\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate the model on validation set after each epoch\n",
    "    print(\"Checking accuracy on Test Data\")\n",
    "    correct_test, samples_test = check_accuracy(val_loader, model)\n",
    "    test_accuracy = 100 * float(correct_test) / float(samples_test)\n",
    "\n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar('Test Accuracy', test_accuracy, epoch) \n",
    "\n",
    "    # Save state of model after each epcoh\n",
    "    torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Aletheia/2_5/al2_5_epoch_{epoch}_correct{correct}.pth')\n",
    "\n",
    "# Stop EmmisionsTracker\n",
    "emissions = carbon_tracker.stop()\n",
    "\n",
    "# Display total carbon emissions\n",
    "print(f\"Emissions: {emissions:.10f} kgCO2eq\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test state_dict of Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdjustedResCNN().to(device)\n",
    "\n",
    "# Create a dummy input and perform a forward pass to create the fc1 layer\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Aletheia/2_5/al2_5_epoch_16_correct235028.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of trained model on the test data\n",
    "check_accuracy(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model using Netron\n",
    "netron.start(\"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Aletheia/2_5/al2_5_epoch_16_correct235028.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Specific Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "new_root_directory = \"C:\\\\Users\\\\jacob\\\\OneDrive\\\\StyleGAN\"\n",
    "\n",
    "new_full_dataset = datasets.ImageFolder(root=new_root_directory)\n",
    "\n",
    "# Apply data augmentation and images transformations\n",
    "new_test_dataset = CustomDataset(\n",
    "    new_full_dataset, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the new dataset\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate accuracy on new dataset\n",
    "check_accuracy(new_test_loader, model, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Custom Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model that will be used to predict individual images\n",
    "model_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/StateDicts/Aletheia/2_5/al2_5_epoch_16_correct235028.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_transforms():\n",
    "    \"\"\"\n",
    "    Combine torchvision and albumentations transforms for an individual image\n",
    "    \"\"\"\n",
    "\n",
    "    tv_transform = tv_transform = TorchvisionBridge(torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224))]))\n",
    "    alb_transform = AlbumentationsTransform(test_augmentation)\n",
    "    \n",
    "    # Apply both transformations to the given image\n",
    "    def combined_transforms(img):\n",
    "        img = tv_transform(img)\n",
    "        return alb_transform(img)\n",
    "    \n",
    "    return combined_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function for getting predictions on a single image\n",
    "from helper_functions import predict_single_image\n",
    "# Import function for displaying predictions on multiple images\n",
    "from helper_functions import display_folder_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/EvalData/Aletheia/ai.jpg\"\n",
    "\n",
    "# Use predict_single_imge function to predict a single image\n",
    "predicted_label = predict_single_image(\n",
    "    img_path, \n",
    "    model,\n",
    "    # Use Albumentations to transform the image as needed\n",
    "    AlbumentationsTransform(test_augmentation)\n",
    ")\n",
    "\n",
    "# Print prediction for the given image\n",
    "print(f\"Predicted probability for image: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined transformations and display images with their predicted probabilities\n",
    "combined_transforms = single_image_transforms()\n",
    "display_folder_images(\"/Users/jacob/OneDrive/Desktop/SyntheticEyeLocal/EvalData/Aletheia/MultipleEval/\", model, combined_transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
