{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "# Albumentations for Data Augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# PIL for image operations\n",
    "from PIL import Image\n",
    "\n",
    "# Matplotlib for plotting and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# TensorBoardX - TensorBoard for PyTorch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# CodeCarbon for tracking our carbon emissions\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "# tqdm for showing progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dimensions of Images\n",
    "This this is so we can better understand our data and helps us to decide which fixed image size to choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of ai-generated images\n",
    "\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia/Version2_4/Dataset/Fake/\"\n",
    "\n",
    "# Getting all images in the directory that contains ai-generated faces\n",
    "img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('jpeg', 'jpg', 'png', 'webp'))]\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# Loop through each image and extract its dimensions\n",
    "for img_file in img_files:\n",
    "    with Image.open(os.path.join(img_dir, img_file)) as img:\n",
    "        width, height = img.size\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "\n",
    "# Plot the dimensions of the images in a scatter plot\n",
    "plt.scatter(widths, heights, alpha=0.5)\n",
    "plt.title('Fake Image Dimensions')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting dimensions of real images\n",
    "\n",
    "img_dir = \"/Users/jacob/OneDrive/Desktop/Aletheia/Version2_4/Dataset/Real/\"\n",
    "\n",
    "# Getting all images in the directory that contains ai-generated faces\n",
    "img_files = [f for f in os.listdir(img_dir) if f.lower().endswith(('jpeg', 'jpg', 'png', 'webp'))]\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "# Loop through each image and extract its dimensions\n",
    "for img_file in img_files:\n",
    "    with Image.open(os.path.join(img_dir, img_file)) as img:\n",
    "        width, height = img.size\n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "\n",
    "# Plot the dimensions of the images in a scatter plot\n",
    "plt.scatter(widths, heights, alpha=0.5)\n",
    "plt.title('Fake Image Dimensions')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simpler CNN model architecture we used in the beginning.\n",
    "    This model is fast, but its performance not always optimal.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, dropout_prob=0.3):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Applying dropout with lower probability for convolutional layers\n",
    "        self.dropout_conv = nn.Dropout(p=dropout_prob/3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 64)\n",
    "\n",
    "        # Dropout layer for fully connected layers\n",
    "        self.dropout_fc = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layer 1 with ReLU activation followed by MaxPooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) \n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Convolutional layer 2 with ReLU activation followed by MaxPooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A deeper CNN that's more complex than the original CNN.\n",
    "    This model can detect more features but is also slower.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Convolutional layer 1\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2), # A lower dropout probability for convolutional layers\n",
    "\n",
    "            # Convolutional layer 2\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2), # A lower dropout probability for convolutional layers\n",
    "\n",
    "            # Convolutional layer 3\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2), # A lower dropout probability for convolutional layers\n",
    "\n",
    "            # Convolutional layer 4\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_prob * 0.2) # A lower dropout probability for convolutional layers\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(128, 1)  # Final output layer for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute the size of flattened features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers.\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that consists of a convolutional block and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_prob=0.2):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        # Define main convolutional block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Define skip connection and adapt channels as if needed\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    # Pass input through main block and add skip connection\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        res = self.residual(x)\n",
    "        return out + res\n",
    "\n",
    "class AdjustedResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an adjusted version of our DeeperDNN with slight adjustments and the addition of skip connections.\n",
    "    This is the model architecture of our currently deployed face detection model.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob=0.3):\n",
    "        super(AdjustedResCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers with residual blocks and max-pooling. The dropout probability is reduced for the convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            ResBlock(3, 24, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(24, 48, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(48, 96, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            ResBlock(96, 192, dropout_prob * 0.2),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 448),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(448, 224),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(224, 112),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "\n",
    "            nn.Linear(112, 1) # Final layer with one output for our binary classification problem\n",
    "        )\n",
    "\n",
    "    def feature_size(self):\n",
    "        \"\"\"\n",
    "        Compute size of flattend features after passing through the convolutional layers.\n",
    "        This is useful for determining the input size for the fully connected layers\n",
    "        \"\"\"\n",
    "        return self.conv_layers(torch.zeros(1, 3, 256, 256)).view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "        # Flatten tensor\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # Pass flattened tensor through fully connected layers\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to GPU if available, else use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters\n",
    "num_classes = 2\n",
    "learning_rate = 0.0007\n",
    "batch_size = 16\n",
    "num_epochs = 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load Data\n",
    "- Split Data\n",
    "- Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    \"\"\"\n",
    "    Class to transform images using the Albumentations library\n",
    "    \"\"\"\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply Albumentations transform to the input image and convert the result to a tensor.\n",
    "    def __call__(self, img):\n",
    "        # Convert image to numpy array if needed\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "        \n",
    "        # Convert augmented image to a tensor and normalize pixel values\n",
    "        augmented = self.transform(image=img)\n",
    "        img_tensor = torch.from_numpy(augmented['image'].transpose(2, 0, 1)).float() / 255.0\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchvisionBridge:\n",
    "    \"\"\"\n",
    "    Bridge to convert Torchvision transforms into a format that can be used with the Albumentations library.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    # Apply torchvision transformations and convert the results to a numpy array\n",
    "    def __call__(self, img):\n",
    "        img = self.transform(img)\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations using both Torchvision and Albumentations libraries.\n",
    "\n",
    "# Torchvision transforms\n",
    "torchvision_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=2, translate=(0.025, 0.025)), # This helps making the model more robust on mobile (since users usually upload screenshots wich have a different positional format than the actual image), but a high value can weaken the accuracy on desktop (when used in the original image resultion).  \n",
    "    transforms.Resize((256, 256))  # Resize all images to 256x256\n",
    "])\n",
    "\n",
    "torchvision_resize = transforms.Compose(\n",
    "    transforms.Resize((256, 256))\n",
    ")\n",
    "\n",
    "# Albumentation transforms for data augmentation\n",
    "augmentation = A.Compose([\n",
    "    A.RandomBrightnessContrast(p=0.1),\n",
    "    A.RandomGamma(p=0.15),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.04, rotate_limit=6, p=0.3),\n",
    "    A.GaussNoise(p=0.1),\n",
    "    A.GaussianBlur(p=0.1),\n",
    "    A.PixelDropout(dropout_prob=0.013, p=0.35),\n",
    "])\n",
    "\n",
    "# Albumentation transforms for test data, where images are simply resized without additional transformation\n",
    "test_augmentation = A.Compose([\n",
    "    A.Resize(256, 256)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This Custom Dataset class is used to incorporate Torchvision and Albumentation transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize CustomDataset Object\n",
    "    def __init__(self, dataset, torchvision_transforms=None, albumentations_transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.torchvision_transforms = torchvision_transforms\n",
    "        self.albumentations_transform = AlbumentationsTransform(albumentations_transform)\n",
    "\n",
    "    # Return number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Check if dataset is a subset and get the path and target accordingly\n",
    "        if isinstance(self.dataset, torch.utils.data.Subset):\n",
    "            path, target = self.dataset.dataset.samples[self.dataset.indices[index]]\n",
    "        else:\n",
    "            path, target = self.dataset.samples[index]\n",
    "        \n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Apply torchvision transforms if defined\n",
    "        if self.torchvision_transforms:\n",
    "            img = self.torchvision_transforms(img)\n",
    "        \n",
    "        # Apply albumentations transforms if defined\n",
    "        if self.albumentations_transform:\n",
    "            img = self.albumentations_transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/Users/jacob/OneDrive/Desktop/Aletheia/Version2_4/Dataset/'\n",
    "\n",
    "# Load dataset without transformations\n",
    "full_dataset = datasets.ImageFolder(root=root_directory)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(46)\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_size = int(0.85 * len(full_dataset))  # 85%\n",
    "validation_size = int(0.05 * len(full_dataset))  # 5%\n",
    "test_size = len(full_dataset) - train_size - validation_size  # 10%\n",
    "\n",
    "train_subset, validation_subset, test_subset = random_split(full_dataset, [train_size, validation_size, test_size])\n",
    "\n",
    "# Apply transformations using the CustomDataset class\n",
    "train_dataset = CustomDataset(\n",
    "    train_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=augmentation\n",
    ")\n",
    "\n",
    "val_dataset = CustomDataset(\n",
    "    validation_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    test_subset, \n",
    "    torchvision_transforms=torchvision_transform, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(dataloader, class_names, num_images=24):\n",
    "    \"\"\"Display a grid of images from a dataloader with their labels\"\"\"\n",
    "\n",
    "    # Get a batch of images with labels from dataloader\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10), \n",
    "                             subplot_kw={'xticks':[], 'yticks':[], 'frame_on':False})\n",
    "    \n",
    "    # Add spacing between the images\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    \n",
    "    # Iterate over each axis in order to display images and associated labels\n",
    "    for ax, img, lbl in zip(axes.ravel(), images, labels): \n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(class_names[lbl])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Using the function to display a few images from the training set\n",
    "show_img(train_loader, class_names=full_dataset.classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(3)\n",
    "\n",
    "# Initialize model and transfer it to the GPU (if available)\n",
    "model = AdjustedResCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BCEWithLogitsLoss for our binary classification problem\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "# Define NAdam (a veriant of the Adam optimizer) as our optimizer\n",
    "optimizer = optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "# Define learning rate scheduler to adjust our learning rate\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorBoard summary writer\n",
    "writer = SummaryWriter(f'runs/Aletheia2_4')\n",
    "step = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Function for Determining Accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(data_loader, model):\n",
    "    \"\"\"\n",
    "    Calculate and print accuracy of the model on a given DataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    print(type(data_loader))\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    # Set model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure no gradients are being computed during evaluation\n",
    "    with torch.inference_mode():\n",
    "        for x, y in data_loader:\n",
    "\n",
    "            # Move the data and labels to the appropriate device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Make predictions with the model\n",
    "            scores = model(x)\n",
    "\n",
    "            # Convert logits to predictions\n",
    "            preds = (torch.sigmoid(scores) > 0.45).squeeze(1).long() # The model generally perfomed better on real world problems with a threshold of 0.45\n",
    "\n",
    "            # Update counters based on models predictions\n",
    "            correct += (preds == y).sum().item()\n",
    "            samples += preds.size(0)\n",
    "\n",
    "        print(f'Got {correct} / {samples} correct with an accuracy of {float(correct)/float(samples)*100:.2f}%')\n",
    "\n",
    "    # Set model back to train mode\n",
    "    model.train()\n",
    "\n",
    "    return correct, samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize EmissionsTracker to tracker to monitor carbon emissions using the CodeCarbon library\n",
    "carbon_tracker = EmissionsTracker(project_name=\"Aletheia2_4\", log_level=\"critical\")\n",
    "carbon_tracker.start()\n",
    "\n",
    "# Initialize tracking of correct predictions and total predictions\n",
    "correct = 0\n",
    "samples = 0\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up log interval for recording metrics\n",
    "metrics_interval = 250\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Reset accuracy counters at beginning of each epoch\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    \n",
    "    # Switch model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Train model on each batch of the train_loader and display progress in current epoch using tqdm\n",
    "    for batch_index, (data, targets) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Progress in epoch\"):\n",
    "\n",
    "        # Move data and targets to the device\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        scores = model(data)\n",
    "        scores = scores.squeeze(1)\n",
    "        loss = loss_function(scores.view(-1), targets.float()) # Compute loss based on model's predictions\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert predictions to binary decisions\n",
    "        preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "\n",
    "        # Update accuracy counters\n",
    "        correct += (preds == targets).sum().item()\n",
    "        samples += preds.size(0)\n",
    "        # Calculate accuracy as percentage\n",
    "        accuracy = 100 * correct / samples\n",
    "\n",
    "\n",
    "        if batch_index % metrics_interval == 0:\n",
    "            # Log metrics to TensorBoard\n",
    "            writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_index)\n",
    "            writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_index)\n",
    "            # Print metrics\n",
    "            print(\"Epoch: \", epoch)\n",
    "            print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "\n",
    "\n",
    "    # Print accumulated accuracy for epoch\n",
    "    print(\"Epoch: \", epoch)\n",
    "    print(f'Got {correct} / {samples} correct with an accuracy {accuracy:.2f}% on training data.')\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate the model on validation set after each epoch\n",
    "    print(\"Checking accuracy on Test Data\")\n",
    "    correct_test, samples_test = check_accuracy(val_loader, model)\n",
    "    test_accuracy = 100 * float(correct_test) / float(samples_test)\n",
    "\n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar('Test Accuracy', test_accuracy, epoch) \n",
    "\n",
    "    # Save state of model after each epcoh\n",
    "    torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/SyntheticEye/StateDicts/Aletheia/al2_4model1_epoch_{epoch}_correct{correct}.pth')\n",
    "\n",
    "# Stop EmmisionsTracker\n",
    "emissions = carbon_tracker.stop()\n",
    "\n",
    "# Display total carbon emissions\n",
    "print(f\"Emissions: {emissions:.10f} kgCO2eq\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test state_dict of Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdjustedResCNN().to(device)\n",
    "\n",
    "# Create a dummy input and perform a forward pass to create the fc1 layer\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "model(sample_input)\n",
    "\n",
    "# Specify path to the trained model weights\n",
    "model_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/StateDicts/Aletheia/al2_4model1_epoch_19_correct233899.pth\"\n",
    "\n",
    "# Load trained weights into the model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of trained model on the test data\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Specific Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "new_root_directory = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/EvalData/Aletheia/real_faces/\"\n",
    "new_full_dataset = datasets.ImageFolder(root=new_root_directory)\n",
    "\n",
    "# Apply data augmentation and images transformations\n",
    "new_test_dataset = CustomDataset(\n",
    "    new_full_dataset, \n",
    "    albumentations_transform=test_augmentation\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the new dataset\n",
    "new_test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate accuracy on new dataset\n",
    "check_accuracy(new_test_loader, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on individual Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model that will be used to predict individual images\n",
    "model_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/StateDicts/Aletheia/al2_4model1_epoch_19_correct233899.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_image_transforms():\n",
    "    \"\"\"\n",
    "    Combine torchvision and albumentations transforms for an individual image\n",
    "    \"\"\"\n",
    "\n",
    "    tv_transform = tv_transform = TorchvisionBridge(torchvision.transforms.Compose([torchvision.transforms.Resize((128, 128))]))\n",
    "    alb_transform = AlbumentationsTransform(test_augmentation)\n",
    "    \n",
    "    # Apply both transformations to the given image\n",
    "    def combined_transforms(img):\n",
    "        img = tv_transform(img)\n",
    "        return alb_transform(img)\n",
    "    \n",
    "    return combined_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(img_path, model, transforms):\n",
    "    \"\"\"\n",
    "    Predicts the label for a single image using trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Apply combined transforms\n",
    "    img_tensor = transforms(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Set model to evaluation mode and predict image\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        scores = model(img_tensor)\n",
    "        probability = torch.sigmoid(scores).squeeze().item()\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Return computed probability\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/Users/jacob/OneDrive/Desktop/SyntheticEye/EvalData/Aletheia/ai.jpg\"\n",
    "\n",
    "# Use predict_single_imge function to predict a single image\n",
    "predicted_label = predict_single_image(\n",
    "    img_path, \n",
    "    model,\n",
    "    # Use Albumentations to transform the image as needed\n",
    "    AlbumentationsTransform(test_augmentation)\n",
    ")\n",
    "\n",
    "# Print prediction for the given image\n",
    "print(f\"Predicted probability for image: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_folder_images(folder, model, combined_transforms, num_images=50):\n",
    "    \"\"\"\n",
    "    Display images from a parent folder along with their correct label and predicted probability.\n",
    "    This helps us to understand the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Possible classes of images\n",
    "    classes = ['real', 'fake']\n",
    "    images = []\n",
    "\n",
    "    # Iterate through classes and get image paths\n",
    "    for label in classes:\n",
    "        class_folder = os.path.join(folder, label)\n",
    "        # Collect every image file path from the current class directory\n",
    "        img_files = [os.path.join(class_folder, f) for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n",
    "        # Append to all_images list with their label\n",
    "        images.extend([(img, label) for img in img_files[:num_images]])\n",
    "\n",
    "    # Initialize grid of subplots to display images\n",
    "    fig, axis = plt.subplots(5, len(images) // 5, figsize=(25, 15))\n",
    "\n",
    "    # Display each image with label and predicted probability\n",
    "    for i, (img_path, correct_label) in enumerate(images):\n",
    "        # Get predicted probability\n",
    "        predicted_probability = predict_single_image(img_path, model, combined_transforms)\n",
    "        \n",
    "        # Load image with PIL\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # Display image\n",
    "        row = i // (len(images) // 5)  \n",
    "        column = i % (len(images) // 5) \n",
    "        axis[row, column].imshow(img)\n",
    "        axis[row, column].set_title(f\"Correct: {correct_label}\\nPred: {predicted_probability:.3f}\")\n",
    "        axis[row, column].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load combined transformations and display images with their predicted probabilities\n",
    "combined_transforms = single_image_transforms()\n",
    "display_folder_images(\"/Users/jacob/OneDrive/Desktop/SyntheticEye/EvalData/Aletheia/MultipleEval/\", model, combined_transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
