{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch specific imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Other imports\n",
    "import kaggle\n",
    "import albumentations as A # - This was installed via pip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ConvNet could be used instead of the \"ComplexCNN\". It has less layers, and likely can detect less features, but is also less likely to overfit and faster\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout_prob=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Dropout layer after convolutional layers (optional)\n",
    "        self.dropout_conv = nn.Dropout(p=dropout_prob/2)  # Typically, dropout in convolutional layers is kept lower\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 64)  # After 2 max-pooling operations, the size becomes (28/2/2 = 7)\n",
    "\n",
    "        # Dropout layer for fully connected layers\n",
    "        self.dropout_fc = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layer 1 with ReLU activation followed by MaxPooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # Reducing spatial dimensions by half\n",
    "        x = self.dropout_conv(x)  # Dropout after pooling (optional)\n",
    "\n",
    "        # Conv layer 2 with ReLU activation followed by MaxPooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # Reducing spatial dimensions by half\n",
    "        x = self.dropout_conv(x)  # Dropout after pooling (optional)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)  # Dropout after activation\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ConvNet is more complex with more layers than the \"standard\" ConvNet defined above\n",
    "class ComplexCNN(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.5):\n",
    "        super(ComplexCNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Dropout after convolutional layers\n",
    "        self.dropout_conv = nn.Dropout(p=dropout_prob/2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)  # Only one output node for binary classification\n",
    "\n",
    "        # Dropout for fully connected layers\n",
    "        self.dropout_fc = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layer 1\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Conv layer 2\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Conv layer 3\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.dropout_conv(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # FC layers\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout_fc(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- Load Data\n",
    "- Split Data\n",
    "- Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the images to tensors without data augmentation\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform:\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Convert PIL image to numpy array\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Apply transformations\n",
    "        augmented = self.transform(image=image_np)\n",
    "\n",
    "        # Convert numpy image back to PyTorch tensor, normalize and permute\n",
    "        image_tensor = torch.tensor(augmented['image'], dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using data augmentation with the albumentations library\n",
    "augmentation = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.RandomGamma(p=0.3),\n",
    "    A.CLAHE(p=0.3),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "    A.GaussNoise(p=0.1),\n",
    "    A.GaussianBlur(p=0.1),\n",
    "    A.OpticalDistortion(p=0.05),\n",
    "    A.GridDistortion(p=0.05),\n",
    "    # A.ToTensorV2()  # Convert the image to PyTorch tensor. It should be the last transform.\n",
    "])\n",
    "\n",
    "test_augmentation = A.Compose([\n",
    "    A.Resize(128, 128)\n",
    "])\n",
    "\n",
    "wrapped_augmentation = transforms.Compose([\n",
    "    AlbumentationsTransform(augmentation),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "wrapped_test_augmentation = transforms.Compose([\n",
    "    AlbumentationsTransform(test_augmentation),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_augmentation = AlbumentationsTransform(augmentation)\n",
    "wrapped_test_augmentation = AlbumentationsTransform(test_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageFolder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomDataset\u001b[39;00m(ImageFolder):\n\u001b[0;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root, transforms\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m         \u001b[39msuper\u001b[39m(CustomDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)  \u001b[39m# We won't use the base class's transform\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ImageFolder' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomDataset(ImageFolder):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        super(CustomDataset, self).__init__(root, transform=None)  # We won't use the base class's transform\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get an image and its label\n",
    "        path, target = self.samples[index]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # Apply albumentations\n",
    "        if self.augmentations:\n",
    "            image = self.augmentations(image=np.array(image))[\"image\"]\n",
    "        \n",
    "        # Convert image back to torch tensor\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_subset, test_subset \u001b[39m=\u001b[39m random_split(full_dataset, [train_size, test_size])\n\u001b[0;32m     10\u001b[0m \u001b[39m# Apply the transformations using the CustomDataset class\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_dataset \u001b[39m=\u001b[39m CustomDataset(train_subset, transform\u001b[39m=\u001b[39mwrapped_augmentation)\n\u001b[0;32m     12\u001b[0m test_dataset \u001b[39m=\u001b[39m CustomDataset(test_subset, transform\u001b[39m=\u001b[39mwrapped_test_augmentation)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Create data loaders\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CustomDataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the entire dataset without any transformations\n",
    "full_dataset = datasets.ImageFolder(root='/Users/jacob/OneDrive/Desktop/Aletheia/Dataset_1/')\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_size = int(0.8 * len(full_dataset))  # 80%\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_subset, test_subset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Apply the transformations using the CustomDataset class\n",
    "train_dataset = CustomDataset(train_subset, transform=wrapped_augmentation)\n",
    "test_dataset = CustomDataset(test_subset, transform=wrapped_test_augmentation)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64  # Adjust as per your need\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample_img, sample_label \u001b[39m=\u001b[39m train_dataset[\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sample_img\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "sample_img, sample_label = train_dataset[0]\n",
    "print(sample_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Get the first five images and their labels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m images \u001b[39m=\u001b[39m [train_dataset[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n\u001b[0;32m      3\u001b[0m labels \u001b[39m=\u001b[39m [train_dataset[i][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Get the first five images and their labels\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m images \u001b[39m=\u001b[39m [train_dataset[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n\u001b[0;32m      3\u001b[0m labels \u001b[39m=\u001b[39m [train_dataset[i][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the first five images and their labels\n",
    "images = [train_dataset[i][0] for i in range(5)]\n",
    "labels = [train_dataset[i][1] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to PIL Images for plotting\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Plotting\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m i, img \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(images):\n\u001b[0;32m      5\u001b[0m     ax \u001b[39m=\u001b[39m axs[i \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m5\u001b[39m][i \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m]\n\u001b[0;32m      6\u001b[0m     ax\u001b[39m.\u001b[39mimshow(to_pil(img))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 5, figsize=(15, 5))\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    ax = axs[i // 5][i % 5]\n",
    "    ax.imshow(to_pil(img))\n",
    "    ax.set_title(f\"Label: {labels[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = ComplexCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'runs/MNIST/tryingout_tensorboard')\n",
    "step = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m num_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      2\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m46\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m      6\u001b[0m log_interval \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_samples = 0\n",
    "torch.manual_seed(46)\n",
    "\n",
    "model = model.to(device=device)\n",
    "log_interval = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    # Move data and targets to the device\n",
    "    data = data.to(device=device)\n",
    "    targets = targets.to(device=device)\n",
    "\n",
    "    # Forward pass\n",
    "    scores = model(data)\n",
    "    scores = scores.squeeze(1)\n",
    "    loss = criterion(scores.view(-1), targets.float())\n",
    "    # print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient descent (or whatever optimizer you're using)\n",
    "    optimizer.step()\n",
    "\n",
    "    predictions = (torch.sigmoid(scores) > 0.5).float()\n",
    "    num_correct += (predictions == targets).sum().item()\n",
    "    num_samples += predictions.size(0)\n",
    "\n",
    "    accuracy = 100 * num_correct / num_samples\n",
    "\n",
    "    # Log the metrics to TensorBoard\n",
    "    if batch_idx % log_interval == 0:\n",
    "      writer.add_scalar('Training Loss', loss, epoch * len(train_loader) + batch_idx)\n",
    "      writer.add_scalar('Training Accuracy', accuracy, epoch * len(train_loader) + batch_idx)\n",
    "      print(\"Epoch: \", epoch)\n",
    "      print(f'Got {num_correct} / {num_samples} with accuracy {accuracy:.2f}')\n",
    "\n",
    "  print(\"~!!1!!!!!!Epoch: \", epoch)\n",
    "\n",
    "  print(f'!!!!!!!!!!Got {num_correct} / {num_samples} with accuracy {accuracy:.2f}')\n",
    "\n",
    "torch.save(model.state_dict(), f'/Users/jacob/OneDrive/Desktop/Aletheia/Version1_0/state_dicts/model_epoch_{epoch}_num_correct{num_correct}.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Got 50179 / 54742 with accuracy 91.66\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    print(type(loader))\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            predictions = (torch.sigmoid(scores) > 0.5).squeeze().long()\n",
    "            num_correct += (predictions == y).sum().item()\n",
    "            num_samples += predictions.size(0)\n",
    "            # print(f\"y shape: {y.shape}\")\n",
    "            # print(f\"y values: {y.unique()}\")\n",
    "            # print(f\"predictions shape: {predictions.shape}\")\n",
    "            # print(f\"predictions values: {predictions.unique()}\")\n",
    "\n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "# check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
